{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Level 3: Advanced Architecture & Interpretability\n",
                "\n",
                "**Objective**: Design custom/advanced architecture and implement interpretability (Grad-CAM).\n",
                "\n",
                "**Architecture**: **EfficientNet-B0** (Fine-tuned). EfficientNet achieves much better accuracy/parameter effiency than ResNet.\n",
                "**Interpretability**: **Grad-CAM** (Gradient-weighted Class Activation Mapping) to visualize where the model looks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms, models\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from sklearn.model_selection import train_test_split\n",
                "import matplotlib.pyplot as plt\n",
                "try:\n",
                "    import cv2\n",
                "except ImportError:\n",
                "    print(\"OpenCV not found. Installing...\")\n",
                "    !pip install opencv-python\n",
                "    import cv2\n",
                "\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Setup (Reusing Level 2 Augmentation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 345M/345M [00:34<00:00, 9.97MB/s] \n",
                        "100%|██████████| 502/502 [00:00<00:00, 91.0kB/s]\n",
                        "100%|██████████| 15.0k/15.0k [00:00<00:00, 3.14MB/s]\n"
                    ]
                }
            ],
            "source": [
                "class MergedFlowersDataset(Dataset):\n",
                "    def __init__(self, image_files, labels, transform=None):\n",
                "        self.image_files = image_files\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "        self.loader = datasets.folder.default_loader\n",
                "    def __len__(self): return len(self.image_files)\n",
                "    def __getitem__(self, idx):\n",
                "        try:\n",
                "            return self.transform(self.loader(self.image_files[idx])), self.labels[idx]\n",
                "        except Exception as e:\n",
                "            print(f\"Error loading {self.image_files[idx]}: {e}\")\n",
                "            raise e\n",
                "\n",
                "def get_data(num_workers=0):\n",
                "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
                "    train_tf = transforms.Compose([\n",
                "        transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(),\n",
                "        transforms.ColorJitter(0.2,0.2,0.2,0.1), transforms.ToTensor(), transforms.Normalize(mean, std)\n",
                "    ])\n",
                "    val_tf = transforms.Compose([\n",
                "        transforms.Resize((256,256)), transforms.CenterCrop(224),\n",
                "        transforms.ToTensor(), transforms.Normalize(mean, std)\n",
                "    ])\n",
                "    \n",
                "    all_samples, all_labels = [], []\n",
                "    for s in ['train', 'val', 'test']:\n",
                "        try:\n",
                "            ds = datasets.Flowers102('./data', split=s, download=True)\n",
                "            all_samples.extend(ds._image_files); all_labels.extend(ds._labels)\n",
                "        except: pass\n",
                "    \n",
                "    idx = np.arange(len(all_samples))\n",
                "    train_i, tmp = train_test_split(idx, test_size=0.2, stratify=all_labels, random_state=42)\n",
                "    val_i, test_i = train_test_split(tmp, test_size=0.5, stratify=np.array(all_labels)[tmp], random_state=42)\n",
                "    \n",
                "    dds = {\n",
                "        'train': MergedFlowersDataset([all_samples[i] for i in train_i], [all_labels[i] for i in train_i], train_tf),\n",
                "        'val': MergedFlowersDataset([all_samples[i] for i in val_i], [all_labels[i] for i in val_i], val_tf),\n",
                "        'test': MergedFlowersDataset([all_samples[i] for i in test_i], [all_labels[i] for i in test_i], val_tf)\n",
                "    }\n",
                "    \n",
                "    loaders = {x: DataLoader(dds[x], batch_size=32, shuffle=(x=='train'), num_workers=num_workers) for x in dds}\n",
                "    return loaders, {x: len(dds[x]) for x in dds}\n",
                "\n",
                "dataloaders, sizes = get_data(num_workers=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Advanced Architecture: EfficientNet-B0\n",
                "We use EfficientNet because it scales dimensions (depth/width/resolution) uniformly, offering better accuracy-to-compute ratio than ResNet."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\sansk\\PROJECTS\\terafac\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
                        "  warnings.warn(\n",
                        "c:\\Users\\sansk\\PROJECTS\\terafac\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
                        "  warnings.warn(msg)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\sansk/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 20.5M/20.5M [00:01<00:00, 15.7MB/s]\n"
                    ]
                }
            ],
            "source": [
                "def build_efficientnet():\n",
                "    model = models.efficientnet_b0(pretrained=True)\n",
                "    \n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = True\n",
                "   \n",
                "    num_ftrs = model.classifier[1].in_features\n",
                "    model.classifier[1] = nn.Linear(num_ftrs, 102)\n",
                "    \n",
                "    model = model.to(device)\n",
                "    return model\n",
                "\n",
                "model_eff = build_efficientnet()\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.AdamW(model_eff.parameters(), lr=1e-4, weight_decay=1e-4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/15\n",
                        "train Acc: 0.2929\n",
                        "val Acc: 0.6398\n",
                        "Epoch 2/15\n",
                        "train Acc: 0.6704\n",
                        "val Acc: 0.8584\n",
                        "Epoch 3/15\n",
                        "train Acc: 0.8266\n",
                        "val Acc: 0.9316\n",
                        "Epoch 4/15\n",
                        "train Acc: 0.8848\n",
                        "val Acc: 0.9499\n",
                        "Epoch 5/15\n",
                        "train Acc: 0.9112\n",
                        "val Acc: 0.9658\n",
                        "Epoch 6/15\n",
                        "train Acc: 0.9246\n",
                        "val Acc: 0.9683\n",
                        "Epoch 7/15\n",
                        "train Acc: 0.9315\n",
                        "val Acc: 0.9731\n",
                        "Epoch 8/15\n"
                    ]
                }
            ],
            "source": [
                "def train(model, epochs=15):\n",
                "    best_acc = 0.0\n",
                "    os.makedirs('models', exist_ok=True)\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
                "        for phase in ['train', 'val']:\n",
                "            if phase == 'train': model.train()\n",
                "            else: model.eval()\n",
                "            \n",
                "            running_acc = 0.0\n",
                "            running_corrects = 0\n",
                "            total_samples = 0\n",
                "            \n",
                "            for x, y in dataloaders[phase]:\n",
                "                x, y = x.to(device), y.to(device)\n",
                "                optimizer.zero_grad()\n",
                "                with torch.set_grad_enabled(phase=='train'):\n",
                "                    out = model(x)\n",
                "                    _, pred = torch.max(out, 1)\n",
                "                    loss = criterion(out, y)\n",
                "                    if phase=='train': \n",
                "                        loss.backward()\n",
                "                        optimizer.step()\n",
                "                \n",
                "                running_corrects += torch.sum(pred == y).item()\n",
                "                total_samples += y.size(0)\n",
                "            \n",
                "            epoch_acc = running_corrects / total_samples\n",
                "            print(f\"{phase} Acc: {epoch_acc:.4f}\")\n",
                "            if phase == 'val' and epoch_acc > best_acc:\n",
                "                best_acc = epoch_acc\n",
                "                torch.save(model.state_dict(), 'models/level_3_efficientnet.pth')\n",
                "    return model\n",
                "\n",
                "# Training\n",
                "model_eff = train(model_eff, epochs=15)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Grad-CAM Visualization\n",
                "Visualizing the last convolutional layer activations to see which flower parts the model focuses on."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GradCAM:\n",
                "    def __init__(self, model, target_layer):\n",
                "        self.model = model\n",
                "        self.target_layer = target_layer\n",
                "        self.gradients = None\n",
                "        self.activations = None\n",
                "        \n",
                "        target_layer.register_forward_hook(self.save_activation)\n",
                "        target_layer.register_backward_hook(self.save_gradient)\n",
                "\n",
                "    def save_activation(self, module, input, output):\n",
                "        self.activations = output\n",
                "\n",
                "    def save_gradient(self, module, grad_input, grad_output):\n",
                "        self.gradients = grad_output[0]\n",
                "\n",
                "    def __call__(self, x, class_idx=None):\n",
                "        self.model.zero_grad()\n",
                "        output = self.model(x)\n",
                "        if class_idx is None:\n",
                "            class_idx = torch.argmax(output)\n",
                "            \n",
                "        output[0, class_idx].backward()\n",
                "        \n",
                "        gradients = self.gradients.data.cpu().numpy()[0]\n",
                "        activations = self.activations.data.cpu().numpy()[0]\n",
                "        \n",
                "        weights = np.mean(gradients, axis=(1, 2))\n",
                "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
                "        \n",
                "        for i, w in enumerate(weights):\n",
                "            cam += w * activations[i]\n",
                "            \n",
                "        cam = np.maximum(cam, 0)\n",
                "        cam = cv2.resize(cam, (224, 224))\n",
                "        cam = cam - np.min(cam)\n",
                "        cam = cam / np.max(cam)\n",
                "        return cam\n",
                "\n",
                "target_layer = model_eff.features[-1] \n",
                "grad_cam = GradCAM(model_eff, target_layer)\n",
                "\n",
                "img_tensor, label = next(iter(dataloaders['test']))\n",
                "img_tensor = img_tensor[0:1].to(device)\n",
                "\n",
                "heatmap = grad_cam(img_tensor)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.title(\"Original\")\n",
                "\n",
                "disp_img = img_tensor.cpu().squeeze().permute(1, 2, 0).numpy()\n",
                "disp_img = disp_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
                "disp_img = np.clip(disp_img, 0, 1)\n",
                "plt.imshow(disp_img)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.title(\"Grad-CAM Heatmap\")\n",
                "plt.imshow(disp_img)\n",
                "plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
