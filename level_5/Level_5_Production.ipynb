{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Level 5: Production System & Optimization\n",
                "\n",
                "**Objective**: Build a production-ready system with deployment optimization.\n",
                "\n",
                "**Techniques**:\n",
                "- **Model Quantization**: Compressing model weights to INT8 to reduce size and speed up CPU inference.\n",
                "- **ONNX Export**: Converting PyTorch model to ONNX format for cross-platform deployment.\n",
                "- **Benchmarking**: Measuring Latency (<100ms goal) and Model Size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torchvision import models, datasets, transforms\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "try:\n",
                "    import onnx\n",
                "    import onnxruntime\n",
                "except ImportError:\n",
                "    print(\"Installing ONNX libraries...\")\n",
                "    !pip install onnx onnxruntime\n",
                "    import onnx\n",
                "    import onnxruntime\n",
                "\n",
                "device = torch.device(\"cpu\") \n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Trained EfficientNet (Level 3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_efficientnet():\n",
                "    model = models.efficientnet_b0(pretrained=False)\n",
                "    num_ftrs = model.classifier[1].in_features\n",
                "    model.classifier[1] = nn.Linear(num_ftrs, 102)\n",
                "    return model\n",
                "\n",
                "model = build_efficientnet()\n",
                "model_path = None\n",
                "possible_paths = [\n",
                "    '../level_3/models/level_3_efficientnet.pth', \n",
                "    'models/level_3_efficientnet.pth',\n",
                "    '../models/level_3_efficientnet.pth'\n",
                "]\n",
                "\n",
                "for p in possible_paths:\n",
                "    if os.path.exists(p):\n",
                "        model_path = p\n",
                "        break\n",
                "\n",
                "if model_path:\n",
                "    print(f\"Loading model from {model_path}\")\n",
                "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
                "else:\n",
                "    print(\"WARNING: Trained model not found. Using random weights for demonstration.\")\n",
                "\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Compression: Dynamic Quantization\n",
                "We convert weights from Float32 to Int8. This reduces memory usage significantly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "quantized_model = torch.quantization.quantize_dynamic(\n",
                "    model, \n",
                "    {nn.Linear},  \n",
                "    dtype=torch.qint8\n",
                ")\n",
                "\n",
                "print(f\"Original Model Size: {os.path.getsize(model_path)/1e6:.2f} MB\" if model_path else \"N/A\")\n",
                "os.makedirs('models', exist_ok=True)\n",
                "torch.save(quantized_model.state_dict(), 'models/level_5_quantized.pth')\n",
                "print(f\"Quantized Model Size: {os.path.getsize('models/level_5_quantized.pth')/1e6:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ONNX Export\n",
                "Standard exchange format for deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dummy_input = torch.randn(1, 3, 224, 224)\n",
                "onnx_path = \"models/level_5_model.onnx\"\n",
                "\n",
                "torch.onnx.export(\n",
                "    model,\n",
                "    dummy_input,\n",
                "    onnx_path,\n",
                "    verbose=False,\n",
                "    input_names=['input'],\n",
                "    output_names=['output'],\n",
                "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
                ")\n",
                "print(f\"Model exported to {onnx_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Latency Benchmarking\n",
                "Testing inference speed (Target < 100ms)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark(model, input_tensor, name=\"Model\", runs=100):\n",
                "    with torch.no_grad():\n",
                "        for _ in range(10):\n",
                "            _ = model(input_tensor)\n",
                "    \n",
                "    start = time.time()\n",
                "    with torch.no_grad():\n",
                "        for _ in range(runs):\n",
                "            _ = model(input_tensor)\n",
                "    end = time.time()\n",
                "    \n",
                "    avg_time = (end - start) / runs * 1000 # ms\n",
                "    print(f\"{name} Inference Time: {avg_time:.2f} ms\")\n",
                "    return avg_time\n",
                "\n",
                "def benchmark_onnx(path, name=\"ONNX\", runs=100):\n",
                "    session = onnxruntime.InferenceSession(path)\n",
                "    input_name = session.get_inputs()[0].name\n",
                "    \n",
                "    x_numpy = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
                "    for _ in range(10):\n",
                "        _ = session.run(None, {input_name: x_numpy})\n",
                "        \n",
                "    start = time.time()\n",
                "    for _ in range(runs):\n",
                "         _ = session.run(None, {input_name: x_numpy})\n",
                "    end = time.time()\n",
                "    \n",
                "    avg_time = (end - start) / runs * 1000\n",
                "    print(f\"{name} Inference Time: {avg_time:.2f} ms\")\n",
                "    return avg_time\n",
                "\n",
                "print(\"--- Benchmarking on CPU ---\")\n",
                "t_orig = benchmark(model, dummy_input, \"Original PyTorch\")\n",
                "t_quant = benchmark(quantized_model, dummy_input, \"Quantized PyTorch\")\n",
                "t_onnx = benchmark_onnx(onnx_path, \"ONNX Runtime\")\n",
                "\n",
                "print(\"\\n--- Results ---\")\n",
                "if t_onnx < 100:\n",
                "    print(\"Success: < 100ms Inference Time achieved!\")\n",
                "else:\n",
                "    print(\"Warning: > 100ms. Consider smaller model or GPU.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
